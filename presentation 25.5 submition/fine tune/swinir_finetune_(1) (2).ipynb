{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "qIesgwcoe0uT",
        "z2gYcynBNmul",
        "T5dwnvPhN10-"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76bedc8156364f4eb89b111aef46dbf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53769d01d2c147259265ad1344fc0b99",
              "IPY_MODEL_e1a25eff9fad4fd5846c6f11589228b7",
              "IPY_MODEL_4db807f6743d47b39707fb15d1058b1c"
            ],
            "layout": "IPY_MODEL_c372be4b064542fab607482217a962da"
          }
        },
        "53769d01d2c147259265ad1344fc0b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e698f8bbf7848f19fd1c5a4fcac7e8d",
            "placeholder": "​",
            "style": "IPY_MODEL_576725a716014c71a5daafc4cbef36b8",
            "value": "Epoch 14: 100%"
          }
        },
        "e1a25eff9fad4fd5846c6f11589228b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbdfb977907340089e2dca95342192f0",
            "max": 630,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6e09bc353964247a2a390999a42b886",
            "value": 630
          }
        },
        "4db807f6743d47b39707fb15d1058b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1c7e8df85145c38346e342e3e9f2e8",
            "placeholder": "​",
            "style": "IPY_MODEL_1bbb3450f4754ba49378ee0d2f351b19",
            "value": " 630/630 [12:51&lt;00:00,  0.82it/s, v_num=1, train_loss=0.125]"
          }
        },
        "c372be4b064542fab607482217a962da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "3e698f8bbf7848f19fd1c5a4fcac7e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576725a716014c71a5daafc4cbef36b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbdfb977907340089e2dca95342192f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e09bc353964247a2a390999a42b886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc1c7e8df85145c38346e342e3e9f2e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bbb3450f4754ba49378ee0d2f351b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# starting up"
      ],
      "metadata": {
        "id": "qIesgwcoe0uT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7F15xvIL8cZ",
        "outputId": "f818e4bb-f089-4bdd-c2d6-0a192191f7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning==2.2.0\n",
            "  Downloading pytorch_lightning-2.2.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.2.0) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.2.0) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.2.0) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning==2.2.0)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.2.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.2.0) (4.13.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning==2.2.0)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning==2.2.0) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->pytorch-lightning==2.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->pytorch-lightning==2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->pytorch-lightning==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->pytorch-lightning==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.2.0) (3.10)\n",
            "Downloading pytorch_lightning-2.2.0-py3-none-any.whl (800 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.2.0 torchmetrics-1.7.1\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2025.3.2)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.14.3)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.7.1)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.13.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
            "Downloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning\n",
            "Successfully installed lightning-2.5.1.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning==2.2.0\n",
        "!pip install rasterio\n",
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. environment flags (must be set *before* importing PL) -----------\n",
        "import os\n",
        "os.environ[\"PL_DISABLE_MIXED_IMPORTS\"] = \"1\"   # use *only* pytorch_lightning\n",
        "os.environ[\"TORCH_NAN_INF_CHECK\"]    = \"1\"     # raise if any NaN/Inf in fwd/bwd\n",
        "\n",
        "# --- 2. standard & utility packages ------------------------------------\n",
        "import sys, shutil, zipfile, csv\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import rasterio\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# --- 3. PyTorch core ----------------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# --- 4. PyTorch Lightning (legacy namespace only) ----------------------\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "\n",
        "# --- 5. Colab conveniences (only if you’re in Colab) -------------------\n",
        "try:\n",
        "    from google.colab import drive, files\n",
        "except ImportError:\n",
        "    drive = files = None        # not running in Colab → ignore\n"
      ],
      "metadata": {
        "id": "q0uBfO7-MBSu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "drive_tif_path = \"/content/drive/MyDrive/final_project/ONLY_TIF\"\n",
        "local_tif_path = \"/content/ONLY_TIF\"\n",
        "\n",
        "# If the folder already exists in local runtime, remove it first\n",
        "if os.path.exists(local_tif_path):\n",
        "    shutil.rmtree(local_tif_path)\n",
        "\n",
        "# Copy entire folder from Drive to local runtime\n",
        "shutil.copytree(drive_tif_path, local_tif_path)\n",
        "\n",
        "print(f\"Copied entire ONLY_TIF folder to: {local_tif_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6IBsUoGMJEn",
        "outputId": "623cf77f-461c-4517-84a1-51b950e7dbcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Copied entire ONLY_TIF folder to: /content/ONLY_TIF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_triplet_csv(source_dir, output_csv_path):\n",
        "    data = []\n",
        "    id_counter = 1\n",
        "\n",
        "    for root, _, files in os.walk(source_dir):\n",
        "        files = [f for f in files if f.lower().endswith('.tif')]\n",
        "        if not files:\n",
        "            continue\n",
        "\n",
        "        goes1 = goes2 = viirs = None\n",
        "        for f in files:\n",
        "            f_lower = f.lower()\n",
        "            full_path = os.path.join(root, f)\n",
        "            rel_path = os.path.relpath(full_path, source_dir)\n",
        "\n",
        "            if 'geo16' in f_lower:\n",
        "                goes1 = rel_path\n",
        "            elif 'geo17' in f_lower:\n",
        "                goes2 = rel_path\n",
        "            elif 'geo18' in f_lower and goes2 is None:\n",
        "                goes2 = rel_path\n",
        "            elif 'combined' in f_lower:\n",
        "                viirs = rel_path\n",
        "\n",
        "        if goes1 and goes2 and viirs:\n",
        "            data.append({\n",
        "                'id': id_counter,\n",
        "                'goes1_path': os.path.join(source_dir, goes1),\n",
        "                'goes2_path': os.path.join(source_dir, goes2),\n",
        "                'viirs_path': os.path.join(source_dir, viirs),\n",
        "            })\n",
        "            id_counter += 1\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"✅ CSV saved to {output_csv_path} with {len(df)} records.\")\n",
        "\n",
        "# Define paths\n",
        "source_dir = \"/content/ONLY_TIF\"\n",
        "output_csv_path = \"/content/superres_triplets.csv\"\n",
        "\n",
        "make_triplet_csv(source_dir, output_csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz3xRR_ENeDK",
        "outputId": "f1e28c48-29d7-4a13-dbbd-2e730ae0e2f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CSV saved to /content/superres_triplets.csv with 1260 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clone"
      ],
      "metadata": {
        "id": "z2gYcynBNmul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sys.path.append(\"C:/Users/97254/OneDrive - post.bgu.ac.il/Desktop/code4finalproj/SwinIR-main\")\n",
        "# Clone the repo\n",
        "!git clone https://github.com/JingyunLiang/SwinIR.git\n",
        "\n",
        "# Manually install necessary dependencies\n",
        "!pip install timm einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNL5SgXENjom",
        "outputId": "473c15bb-e2b9-4a1d-8f9d-768266d720e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SwinIR'...\n",
            "remote: Enumerating objects: 333, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 333 (delta 6), reused 2 (delta 2), pack-reused 323 (from 2)\u001b[K\n",
            "Receiving objects: 100% (333/333), 29.84 MiB | 22.43 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd SwinIR\n",
        "\n",
        "# Download the grayscale denoising pre-trained weights\n",
        "!mkdir -p experiments/pretrained_models\n",
        "!wget https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth -P experiments/pretrained_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-BfRYfPNu9q",
        "outputId": "0646a027-7301-4778-e561-28f289aa9bd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SwinIR\n",
            "--2025-05-16 09:24:54--  https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/396770997/44b18cfe-3817-49c6-aed0-9f8912acb152?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250516%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250516T092454Z&X-Amz-Expires=300&X-Amz-Signature=ede8d4d67f1255b53b400af12cd5c60357ad7be8afdb1cbea8393d1ff4cabcba&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-16 09:24:54--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/396770997/44b18cfe-3817-49c6-aed0-9f8912acb152?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250516%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250516T092454Z&X-Amz-Expires=300&X-Amz-Signature=ede8d4d67f1255b53b400af12cd5c60357ad7be8afdb1cbea8393d1ff4cabcba&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 122879887 (117M) [application/octet-stream]\n",
            "Saving to: ‘experiments/pretrained_models/004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth’\n",
            "\n",
            "004_grayDN_DFWB_s12 100%[===================>] 117.19M  44.5MB/s    in 2.6s    \n",
            "\n",
            "2025-05-16 09:24:57 (44.5 MB/s) - ‘experiments/pretrained_models/004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth’ saved [122879887/122879887]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dataset"
      ],
      "metadata": {
        "id": "T5dwnvPhN10-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SatelliteImageDataset(Dataset):\n",
        "    def __init__(self, csv_path, json_path, transform=None):\n",
        "        \"\"\"\n",
        "        Dataset for satellite image super-resolution\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to CSV file with image triplets\n",
        "            json_path: Path to JSON file with normalization parameters\n",
        "            transform: Optional transforms to apply to images\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Load normalization parameters from JSON\n",
        "        with open(json_path, 'r') as f:\n",
        "            self.norm_params = json.load(f)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get paths for this sample\n",
        "        sample = self.df.iloc[idx]\n",
        "        goes1_path = sample['goes1_path']\n",
        "        goes2_path = sample['goes2_path']\n",
        "        viirs_path = sample['viirs_path']\n",
        "\n",
        "        # Load images using rasterio\n",
        "        goes1 = self._load_and_process_image(goes1_path, 'goes')\n",
        "        goes2 = self._load_and_process_image(goes2_path, 'goes')\n",
        "        viirs = self._load_and_process_image(viirs_path, 'viirs')\n",
        "\n",
        "        # Apply additional transforms if specified\n",
        "        if self.transform:\n",
        "            goes1 = self.transform(goes1)\n",
        "            goes2 = self.transform(goes2)\n",
        "            viirs = self.transform(viirs)\n",
        "\n",
        "        # Return (goes1, goes2), viirs format\n",
        "        return (goes1, goes2), viirs\n",
        "\n",
        "    def _load_and_process_image(self, path, img_type):\n",
        "        \"\"\"Load, clip and normalize an image\"\"\"\n",
        "        with rasterio.open(path) as src:\n",
        "            # Use band 7 for GOES and band 1 for VIIRS\n",
        "            band_idx = 7 if img_type == 'goes' else 1\n",
        "            img = src.read(band_idx)\n",
        "\n",
        "        # Handle NaN and Inf values\n",
        "        mask = ~(np.isnan(img) | np.isinf(img))\n",
        "        if np.any(mask):\n",
        "            mean_val = img[mask].mean()\n",
        "            img = np.where(mask, img, mean_val)\n",
        "        else:\n",
        "            img = np.zeros_like(img)\n",
        "\n",
        "        # Get correct normalization values from JSON\n",
        "        # JSON has percentile values for GOES and VIIRS\n",
        "        sat_type = \"GOES\" if img_type == \"goes\" else \"VIIRS\"\n",
        "        min_val = self.norm_params[sat_type][\"p2\"]\n",
        "        max_val = self.norm_params[sat_type][\"p98\"]\n",
        "\n",
        "        # Clip values based on image type\n",
        "        img = np.clip(img, min_val, max_val)\n",
        "\n",
        "        # # Normalize to [0, 1]\n",
        "        img = (img - min_val) / (max_val - min_val)\n",
        "\n",
        "        # Convert to PyTorch tensor and add channel dimension\n",
        "        img_tensor = torch.from_numpy(img).float().unsqueeze(0)\n",
        "\n",
        "        return img_tensor\n",
        "\n",
        "\n",
        "\n",
        "class SatelliteDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_path,\n",
        "        json_path,\n",
        "        batch_size=8,\n",
        "        num_workers=0,\n",
        "        transform=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        PyTorch Lightning DataModule for satellite super-resolution\n",
        "        Uses all available data for training only.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to CSV file with image triplets\n",
        "            json_path: Path to JSON file with normalization parameters\n",
        "            batch_size: Batch size for dataloader\n",
        "            num_workers: Number of worker processes for data loading\n",
        "            transform: Optional transforms to apply\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.csv_path = csv_path\n",
        "        self.json_path = json_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.transform = transform\n",
        "        self.save_hyperparameters(ignore=['transform'])\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Create dataset with all available images\n",
        "        self.train_dataset = SatelliteImageDataset(\n",
        "            csv_path=self.csv_path,\n",
        "            json_path=self.json_path,\n",
        "            transform=self.transform\n",
        "        )\n",
        "\n",
        "        print(f\"Training dataset ready with {len(self.train_dataset)} samples\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # No validation data\n",
        "        return None\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # No test data\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "fcY-V4wpN36F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# callbacks"
      ],
      "metadata": {
        "id": "Sz4kRC8odu6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizePredictionCallback(Callback):\n",
        "    def __init__(self, goes1_path, goes2_path, viirs_path, every_n_epochs=1):\n",
        "        super().__init__()\n",
        "        self.goes1_path = goes1_path\n",
        "        self.goes2_path = goes2_path\n",
        "        self.viirs_path = viirs_path\n",
        "        self.every_n_epochs = every_n_epochs\n",
        "\n",
        "        # Load visualization scaling values from JSON\n",
        "        with open(\"/content/radiance_visualization_ranges.json\", \"r\") as f:\n",
        "            self.ranges = json.load(f)\n",
        "\n",
        "        # Make output directory with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        self.output_dir = os.path.join(\"/content/checkpoints\", f\"visual_{timestamp}\")\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def _load_and_normalize_image(self, path, band=1, is_viirs=False):\n",
        "        \"\"\"Load and normalize an image using same method as dataset\"\"\"\n",
        "        with rasterio.open(path) as src:\n",
        "            image = src.read(band).astype(np.float32)\n",
        "\n",
        "        # Handle NaN/Inf values\n",
        "        mask = ~(np.isnan(image) | np.isinf(image))\n",
        "        if np.any(mask):\n",
        "            mean_val = image[mask].mean()\n",
        "            image = np.where(mask, image, mean_val)\n",
        "        else:\n",
        "            image = np.zeros_like(image)\n",
        "\n",
        "        # Normalize using same method as SatelliteImageDataset\n",
        "        sat_type = \"VIIRS\" if is_viirs else \"GOES\"\n",
        "        min_val = self.ranges[sat_type][\"p2\"]\n",
        "        max_val = self.ranges[sat_type][\"p98\"]\n",
        "\n",
        "        # Clip and normalize to [0,1]\n",
        "        image = np.clip(image, min_val, max_val)\n",
        "        image = (image - min_val) / (max_val - min_val)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        epoch = trainer.current_epoch\n",
        "        if epoch % self.every_n_epochs != 0:\n",
        "            return\n",
        "\n",
        "        # Load and normalize images the same way as dataset\n",
        "        goes1_img = self._load_and_normalize_image(self.goes1_path, band=7)\n",
        "        goes2_img = self._load_and_normalize_image(self.goes2_path, band=7)\n",
        "        viirs_img = self._load_and_normalize_image(self.viirs_path, band=1, is_viirs=True)\n",
        "\n",
        "        # Convert to tensors\n",
        "        goes1_tensor = torch.from_numpy(goes1_img).unsqueeze(0).unsqueeze(0).to(pl_module.device)\n",
        "        goes2_tensor = torch.from_numpy(goes2_img).unsqueeze(0).unsqueeze(0).to(pl_module.device)\n",
        "\n",
        "        # Run prediction\n",
        "        with torch.no_grad():\n",
        "            predicted = pl_module(goes1_tensor, goes2_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "            # Debug information\n",
        "            print(f\"Prediction stats - min: {predicted.min():.6f}, max: {predicted.max():.6f}, mean: {predicted.mean():.6f}\")\n",
        "            print(f\"Prediction shape: {predicted.shape}, VIIRS shape: {viirs_img.shape}\")\n",
        "\n",
        "            # If prediction is all near zero, amplify for visualization\n",
        "            if predicted.max() < 0.1:\n",
        "                print(\"Warning: Prediction values are very small - amplifying for visualization\")\n",
        "                # Try to amplify signal for visualization without changing actual model output\n",
        "                viz_predicted = predicted.copy()\n",
        "                if viz_predicted.max() > 0:\n",
        "                    viz_predicted = viz_predicted / viz_predicted.max()  # Normalize to [0,1] for visibility\n",
        "                else:\n",
        "                    viz_predicted = predicted  # If all zeros, don't change\n",
        "            else:\n",
        "                viz_predicted = predicted\n",
        "\n",
        "            # Handle NaN in prediction\n",
        "            viz_predicted = np.nan_to_num(viz_predicted, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "            # Ensure prediction is in [0,1] range\n",
        "            viz_predicted = np.clip(viz_predicted, 0, 1)\n",
        "\n",
        "        # Plot all 4 images side by side\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        titles = [\"GOES-1\", \"GOES-2\", \"VIIRS (GT)\", \"Predicted\"]\n",
        "        images = [goes1_img, goes2_img, viirs_img, viz_predicted]\n",
        "\n",
        "        for ax, img, title in zip(axs, images, titles):\n",
        "            im = ax.imshow(img, cmap=\"gray\", vmin=0, vmax=1.0)\n",
        "            ax.set_title(title)\n",
        "            ax.axis(\"off\")\n",
        "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "        plt.suptitle(f\"Epoch {epoch}\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.output_dir, f\"epoch_{epoch:03d}.png\")\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        print(f\"✅ Saved visualization to {save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "udUSLWaveSUD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PSNRValidationCallback(Callback):\n",
        "    \"\"\"\n",
        "    Compute corrected PSNR (Kelvin metric) on a fixed month of triplets.\n",
        "    Logs cPSNR scores per epoch into a CSV in vis_callback.output_dir\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vis_callback, val_month_dir, every_n_epochs=1):\n",
        "        super().__init__()\n",
        "        self.vis_callback   = vis_callback\n",
        "        self.val_month_dir  = val_month_dir\n",
        "        self.every_n_epochs = every_n_epochs\n",
        "\n",
        "        # Read radiometric range for VIIRS\n",
        "        with open(\"/content/radiance_visualization_ranges.json\") as f:\n",
        "            rng = json.load(f)\n",
        "        self.vi_min = rng[\"VIIRS\"][\"p2\"]\n",
        "        self.vi_rng = rng[\"VIIRS\"][\"p98\"] - self.vi_min\n",
        "\n",
        "        self.goes_min = rng[\"GOES\"][\"p2\"]\n",
        "        self.goes_rng = rng[\"GOES\"][\"p98\"] - self.goes_min\n",
        "\n",
        "        # Collect validation triplets\n",
        "        self.triplets = self._collect_triplets(val_month_dir)\n",
        "        if not self.triplets:\n",
        "            print(\"[WARNING] No validation triplets found — skipping PSNR computation.\")\n",
        "\n",
        "        # CSV output path\n",
        "        self.csv_path = os.path.join(self.vis_callback.output_dir, \"cpsnr_log.csv\")\n",
        "        # Write header if file doesn't exist yet\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"epoch\", \"cpsnr\"])\n",
        "\n",
        "    @staticmethod\n",
        "    def _collect_triplets(root):\n",
        "        out = []\n",
        "        for cur, _, files in os.walk(root):\n",
        "            files = [f for f in files if f.lower().endswith(\".tif\")]\n",
        "            if not files:\n",
        "                continue\n",
        "            g1 = g2 = v = None\n",
        "            for f in files:\n",
        "                p = os.path.join(cur, f)\n",
        "                lf = f.lower()\n",
        "                if \"geo16\" in lf:\n",
        "                    g1 = p\n",
        "                elif \"geo17\" in lf or (\"geo18\" in lf and g2 is None):\n",
        "                    g2 = p\n",
        "                elif \"viirs\" in lf or \"combined\" in lf:\n",
        "                    v = p\n",
        "            if g1 and g2 and v:\n",
        "                out.append((g1, g2, v))\n",
        "            else:\n",
        "                print(f\"[WARNING] Incomplete triplet in {cur} → g1: {bool(g1)}, g2: {bool(g2)}, v: {bool(v)}\")\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_band(path):\n",
        "        is_viirs   = \"viirs\" in path.lower() or \"combined\" in path.lower()\n",
        "        band_index = 1 if is_viirs else 7\n",
        "        with rasterio.open(path) as src:\n",
        "            img = src.read(band_index).astype(np.float32)\n",
        "        m = ~(np.isnan(img) | np.isinf(img))\n",
        "        return np.where(m, img, img[m].mean() if m.any() else 0.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def cpsnr(gt: np.ndarray, pred: np.ndarray, mask: np.ndarray) -> float:\n",
        "        diff = (gt - pred) * mask\n",
        "        b    = diff.sum() / (mask.sum() + 1e-8)\n",
        "        cmse = ((gt - pred + b) ** 2 * mask).sum() / (mask.sum() + 1e-8)\n",
        "        # print(f\"cMSE: {cmse:.4f}, bias: {b:.4f}, cPSNR: {-10.0 * np.log10(cmse + 1e-8):.2f}\")\n",
        "        return -10.0 * np.log10(cmse + 1e-8)\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        epoch = trainer.current_epoch\n",
        "        if (epoch + 1) % self.every_n_epochs:\n",
        "            return\n",
        "\n",
        "        psnrs = []\n",
        "\n",
        "        for g1_path, g2_path, v_path in self.triplets:\n",
        "            g1 = self._load_band(g1_path)\n",
        "            g2 = self._load_band(g2_path)\n",
        "            vi = self._load_band(v_path)\n",
        "\n",
        "            # Normalize ground truth VIIRS and geo pictures\n",
        "            vi_scaled = np.clip((vi - self.vi_min) / self.vi_rng, 0, 1)\n",
        "            g1 = np.clip((g1 - self.goes_min) / self.goes_rng, 0, 1)\n",
        "            g2 = np.clip((g2 - self.goes_min) / self.goes_rng, 0, 1)\n",
        "\n",
        "\n",
        "            # Predict using scaled GOES inputs\n",
        "            pred = pl_module(\n",
        "                torch.from_numpy(g1)[None, None].to(pl_module.device),\n",
        "                torch.from_numpy(g2)[None, None].to(pl_module.device)\n",
        "            ).squeeze().detach().cpu().numpy()\n",
        "\n",
        "            # print(\"GT min/max:\", vi_scaled.min(), vi_scaled.max())\n",
        "            # print(\"Pred min/max:\", pred.min(), pred.max())\n",
        "            # print(\"GOES1 min/max:\", g1.min(), g1.max())\n",
        "            # print(\"GOES2 min/max:\", g2.min(), g2.max())\n",
        "\n",
        "\n",
        "            psnrs.append(self.cpsnr(vi_scaled, pred, np.ones_like(vi_scaled)))\n",
        "\n",
        "        mean_psnr = float(np.mean(psnrs))\n",
        "        if not hasattr(pl_module, \"psnr_scores\"):\n",
        "            pl_module.psnr_scores = []\n",
        "        pl_module.psnr_scores.append(mean_psnr)\n",
        "\n",
        "        # Save PSNR plot\n",
        "        plot_path = os.path.join(self.vis_callback.output_dir, f\"psnr_curve_epoch_{epoch:03d}.png\")\n",
        "        plt.figure()\n",
        "        plt.plot(pl_module.psnr_scores, marker='o')\n",
        "        plt.title(\"Validation cPSNR\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(\"dB\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "\n",
        "        # Append to CSV log\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([epoch, round(mean_psnr, 4)])\n",
        "\n",
        "        print(f\"📈  epoch {epoch:03d}  mean cPSNR: {mean_psnr:.2f} dB\")"
      ],
      "metadata": {
        "id": "cojtVd7KKCW3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossTrackingCallback(pl.Callback):\n",
        "    def __init__(self, vis_callback):\n",
        "        super().__init__()\n",
        "        self.vis_callback = vis_callback\n",
        "        self.losses = []\n",
        "        self.csv_path = os.path.join(vis_callback.output_dir, \"loss_log.csv\")\n",
        "\n",
        "        # Write CSV header once\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"epoch\", \"loss\"])\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        epoch = trainer.current_epoch\n",
        "        loss = float(trainer.callback_metrics[\"train_loss\"])\n",
        "\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Save curve\n",
        "        plt.figure()\n",
        "        plt.plot(self.losses, marker='o')\n",
        "        plt.title(\"Training Loss per Epoch\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plot_path = os.path.join(self.vis_callback.output_dir, f\"loss_curve_epoch_{epoch:03d}.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "\n",
        "        # Save to CSV\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([epoch, round(loss, 6)])\n",
        "\n",
        "        print(f\"📉  epoch {epoch:03d}  loss: {loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "Ie-cFR7JY6as"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modeling"
      ],
      "metadata": {
        "id": "q2QluHeWeVrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sys.path.append('/content/SwinIR')  # Add the repository to path\n",
        "from models.network_swinir import SwinIR\n",
        "\n",
        "def create_swinir_grayscale(\n",
        "    pretrained_path,\n",
        "    img_size=100,\n",
        "    upscale=4\n",
        "):\n",
        "    # Find suitable window size\n",
        "    window_size = 10  # Use fixed window size for now\n",
        "\n",
        "    # Create model specifically for grayscale SR\n",
        "    model = SwinIR(\n",
        "        upscale=upscale,      # 4x upscaling\n",
        "        in_chans=1,           # 1 channel input\n",
        "        img_size=img_size,    # 100x100 input\n",
        "        window_size=window_size,\n",
        "        img_range=1.,         # Normalized range [0,1]\n",
        "        depths=[6, 6, 6, 6, 6, 6],\n",
        "        embed_dim=180,\n",
        "        num_heads=[6, 6, 6, 6, 6, 6],\n",
        "        mlp_ratio=2,\n",
        "        upsampler='nearest+conv',  # Critical for SR\n",
        "        resi_connection='1conv'\n",
        "    )\n",
        "\n",
        "    # Load pretrained weights but DON'T try strict loading\n",
        "    pretrained = torch.load(pretrained_path, map_location='cpu')\n",
        "    if 'params' in pretrained:\n",
        "        pretrained = pretrained['params']\n",
        "\n",
        "    # Only load compatible parameters\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = {k: v for k, v in pretrained.items()\n",
        "                      if k in model_dict and v.shape == model_dict[k].shape}\n",
        "\n",
        "    # Load matching parameters\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict, strict=False)\n",
        "\n",
        "    print(f\"Loaded {len(pretrained_dict)}/{len(model_dict)} parameters from {pretrained_path}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "DNcDqbsyeqCy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinIRGrayscaleLightningModule(pl.LightningModule):\n",
        "    def __init__(self, lr=1e-4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a single SwinIR model with upscaling (upscale=4)\n",
        "        self.swinir_model = create_swinir_grayscale(\n",
        "            pretrained_path='experiments/pretrained_models/004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth',\n",
        "            img_size=100,\n",
        "            upscale=4  # Keep upscaling in the model\n",
        "        )\n",
        "\n",
        "        # Create a deeper fusion network that works on upscaled inputs (400x400)\n",
        "        self.fusion = nn.Sequential(\n",
        "            # Initial fusion of both high-res inputs\n",
        "            nn.Conv2d(2, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            # Deeper processing\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            # Final output layers\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.criterion1 = nn.MSELoss()\n",
        "        self.criterion2 = nn.L1Loss()\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, goes1, goes2):\n",
        "        # Process both inputs through the same SwinIR model (with upscaling)\n",
        "        goes1_upscaled = self.swinir_model(goes1)  # [B, 1, 400, 400]\n",
        "        goes2_upscaled = self.swinir_model(goes2)  # [B, 1, 400, 400]\n",
        "\n",
        "        # Concatenate the upscaled features and apply fusion\n",
        "        x = torch.cat([goes1_upscaled, goes2_upscaled], dim=1)  # [B, 2, 400, 400]\n",
        "        output = self.fusion(x)  # [B, 1, 400, 400]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        (goes1, goes2), viirs = batch\n",
        "\n",
        "        # Forward pass\n",
        "        output = self(goes1, goes2)\n",
        "\n",
        "        # Ensure dimensions match\n",
        "        if output.shape[2:] != viirs.shape[2:]:\n",
        "            output = F.interpolate(output, size=viirs.shape[2:],\n",
        "                                   mode='bilinear', align_corners=False)\n",
        "\n",
        "        loss = self.criterion1(output, viirs)\n",
        "        loss += self.criterion2(output, viirs)\n",
        "        # Add out-of-range penalty\n",
        "        # penalty = torch.mean(torch.relu(output - 1.0) ** 2 + torch.relu(-output) ** 2)\n",
        "        # loss += 0.1 * penalty  # Tune the 0.1 weight\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
      ],
      "metadata": {
        "id": "zABp35_te_IN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# experiment"
      ],
      "metadata": {
        "id": "UBhxSlz9e1-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create visualization callback\n",
        "vis_callback = VisualizePredictionCallback(\n",
        "    goes1_path=\"/content/ONLY_TIF/2020-11/2020-11-01_20-12/clipped_geo16.tif\",\n",
        "    goes2_path=\"/content/ONLY_TIF/2020-11/2020-11-01_20-12/clipped_geo17.tif\",\n",
        "    viirs_path=\"/content/ONLY_TIF/2020-11/2020-11-01_20-12/combined_clip.tif\",\n",
        "    every_n_epochs=1\n",
        ")\n",
        "\n",
        "# Create PSNR validation callback\n",
        "psnr_callback = PSNRValidationCallback(\n",
        "    vis_callback=vis_callback,\n",
        "    val_month_dir=\"/content/ONLY_TIF/2023-02\",\n",
        "    every_n_epochs=1\n",
        ")\n",
        "\n",
        "loss_cb = LossTrackingCallback(vis_callback)\n",
        "\n",
        "\n",
        "\n",
        "# Data module\n",
        "datamodule = SatelliteDataModule(\n",
        "    csv_path=\"/content/superres_triplets.csv\",\n",
        "        json_path=\"/content/radiance_visualization_ranges.json\",\n",
        "        batch_size=2,\n",
        "        num_workers=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "onIi0o97fFPr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model - using grayscale denoising weights with correct dimensions\n",
        "pl_model = SwinIRGrayscaleLightningModule()\n",
        "\n",
        "# Create trainer with callbacks\n",
        "trainer = Trainer(\n",
        "    max_epochs=15,\n",
        "    accelerator='gpu',\n",
        "    devices=1,\n",
        "    precision=32,\n",
        "    log_every_n_steps=10,\n",
        "    callbacks=[psnr_callback, vis_callback, loss_cb]\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.fit(pl_model, datamodule=datamodule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "76bedc8156364f4eb89b111aef46dbf9",
            "53769d01d2c147259265ad1344fc0b99",
            "e1a25eff9fad4fd5846c6f11589228b7",
            "4db807f6743d47b39707fb15d1058b1c",
            "c372be4b064542fab607482217a962da",
            "3e698f8bbf7848f19fd1c5a4fcac7e8d",
            "576725a716014c71a5daafc4cbef36b8",
            "dbdfb977907340089e2dca95342192f0",
            "d6e09bc353964247a2a390999a42b886",
            "dc1c7e8df85145c38346e342e3e9f2e8",
            "1bbb3450f4754ba49378ee0d2f351b19"
          ]
        },
        "id": "R6usIT4dfLgH",
        "outputId": "5d688cd9-a2ca-4de6-da02-c42103c7845e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 453/552 parameters from experiments/pretrained_models/004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth\n",
            "Training dataset ready with 1260 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name         | Type       | Params\n",
            "--------------------------------------------\n",
            "0 | swinir_model | SwinIR     | 11.7 M\n",
            "1 | fusion       | Sequential | 333 K \n",
            "2 | criterion1   | MSELoss    | 0     \n",
            "3 | criterion2   | L1Loss     | 0     \n",
            "--------------------------------------------\n",
            "12.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "12.1 M    Total params\n",
            "48.298    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76bedc8156364f4eb89b111aef46dbf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈  epoch 000  mean cPSNR: 7.38 dB\n",
            "Prediction stats - min: 0.441323, max: 0.677713, mean: 0.631059\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_000.png\n",
            "📉  epoch 000  loss: 0.243662\n",
            "📈  epoch 001  mean cPSNR: 9.46 dB\n",
            "Prediction stats - min: 0.419201, max: 0.709854, mean: 0.556045\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_001.png\n",
            "📉  epoch 001  loss: 0.299726\n",
            "📈  epoch 002  mean cPSNR: 10.67 dB\n",
            "Prediction stats - min: 0.366161, max: 0.760957, mean: 0.612440\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_002.png\n",
            "📉  epoch 002  loss: 0.174907\n",
            "📈  epoch 003  mean cPSNR: 11.19 dB\n",
            "Prediction stats - min: 0.420831, max: 0.671676, mean: 0.564205\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_003.png\n",
            "📉  epoch 003  loss: 0.266926\n",
            "📈  epoch 004  mean cPSNR: 10.73 dB\n",
            "Prediction stats - min: 0.183669, max: 0.774422, mean: 0.586327\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_004.png\n",
            "📉  epoch 004  loss: 0.187894\n",
            "📈  epoch 005  mean cPSNR: 11.26 dB\n",
            "Prediction stats - min: 0.424876, max: 0.669996, mean: 0.586973\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_005.png\n",
            "📉  epoch 005  loss: 0.380995\n",
            "📈  epoch 006  mean cPSNR: 9.98 dB\n",
            "Prediction stats - min: 0.469570, max: 0.744023, mean: 0.617710\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_006.png\n",
            "📉  epoch 006  loss: 0.232584\n",
            "📈  epoch 007  mean cPSNR: 11.25 dB\n",
            "Prediction stats - min: 0.443361, max: 0.713655, mean: 0.595898\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_007.png\n",
            "📉  epoch 007  loss: 0.160334\n",
            "📈  epoch 008  mean cPSNR: 12.16 dB\n",
            "Prediction stats - min: 0.318572, max: 0.725417, mean: 0.616454\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_008.png\n",
            "📉  epoch 008  loss: 0.251373\n",
            "📈  epoch 009  mean cPSNR: 10.29 dB\n",
            "Prediction stats - min: 0.360171, max: 0.853272, mean: 0.722570\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_009.png\n",
            "📉  epoch 009  loss: 0.105276\n",
            "📈  epoch 010  mean cPSNR: 12.83 dB\n",
            "Prediction stats - min: 0.294455, max: 0.735517, mean: 0.620540\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_010.png\n",
            "📉  epoch 010  loss: 0.107270\n",
            "📈  epoch 011  mean cPSNR: 12.18 dB\n",
            "Prediction stats - min: 0.370393, max: 0.808341, mean: 0.667023\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_011.png\n",
            "📉  epoch 011  loss: 0.165815\n",
            "📈  epoch 012  mean cPSNR: 12.16 dB\n",
            "Prediction stats - min: 0.243387, max: 0.872859, mean: 0.693668\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_012.png\n",
            "📉  epoch 012  loss: 0.177927\n",
            "📈  epoch 013  mean cPSNR: 11.24 dB\n",
            "Prediction stats - min: 0.413467, max: 0.745661, mean: 0.607846\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_013.png\n",
            "📉  epoch 013  loss: 0.132520\n",
            "📈  epoch 014  mean cPSNR: 14.95 dB\n",
            "Prediction stats - min: 0.282019, max: 0.658475, mean: 0.509049\n",
            "Prediction shape: (400, 400), VIIRS shape: (400, 400)\n",
            "✅ Saved visualization to /content/checkpoints/visual_2025-05-16_09-51-35/epoch_014.png\n",
            "📉  epoch 014  loss: 0.125320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=15` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make a script that will zip the checkpoints folder and then download the folder\n",
        "\n",
        "# Zip the checkpoints folder\n",
        "zip_filename = \"checkpoints.zip\"\n",
        "!zip -r {zip_filename} /content/checkpoints\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OiX_I8PmrD-5",
        "outputId": "13600c7c-109a-4d41-f8ee-be4d9eb2bdd0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/checkpoints/ (stored 0%)\n",
            "updating: content/checkpoints/visual_2025-05-16_09-25-01/ (stored 0%)\n",
            "updating: content/checkpoints/visual_2025-05-16_09-25-01/loss_log.csv (stored 0%)\n",
            "updating: content/checkpoints/visual_2025-05-16_09-25-01/psnr_curve_epoch_000.png (deflated 24%)\n",
            "updating: content/checkpoints/visual_2025-05-16_09-25-01/loss_curve_epoch_000.png (deflated 23%)\n",
            "updating: content/checkpoints/visual_2025-05-16_09-25-01/cpsnr_log.csv (stored 0%)\n",
            "updating: content/checkpoints/visual_2025-05-16_09-25-01/epoch_000.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/ (stored 0%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_006.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_006.png (deflated 10%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_log.csv (deflated 39%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_001.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_013.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_010.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_008.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_009.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_014.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_004.png (deflated 11%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_001.png (deflated 10%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_000.png (deflated 23%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_012.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_003.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_003.png (deflated 11%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_011.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_012.png (deflated 7%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_005.png (deflated 10%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_013.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_010.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_014.png (deflated 7%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_008.png (deflated 7%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_005.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_013.png (deflated 7%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_011.png (deflated 7%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_006.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_002.png (deflated 11%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_002.png (deflated 10%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_012.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_014.png (deflated 9%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_000.png (deflated 23%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_004.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_001.png (deflated 14%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_009.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_008.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_002.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_010.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_004.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_009.png (deflated 8%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/psnr_curve_epoch_007.png (deflated 9%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_011.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_007.png (deflated 7%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/loss_curve_epoch_003.png (deflated 10%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_007.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/cpsnr_log.csv (deflated 35%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_005.png (deflated 4%)\n",
            "  adding: content/checkpoints/visual_2025-05-16_09-51-35/epoch_000.png (deflated 4%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cff3738b-05ab-4dd2-8935-ccfa5965250f\", \"checkpoints.zip\", 5940515)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}