{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ZEjrXxNmQV"
      },
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-RoiR68DKvGV",
        "outputId": "d27df2f5-2934-452c-ebcc-feee3925f28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.5.1.post0 torchmetrics-1.7.1\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c4KAOGuEEdhe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbZ45kXgEoc-",
        "outputId": "67ca2060-a6e9-4675-cbb6-ac9c7c6923c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "base_folder = '/content/drive/My Drive/ONLY_TIF'\n",
        "csv_path = '/content/drive/My Drive/superres_triplets.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DKFwD2UjFe_J"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "for col in ['goes1_path', 'goes2_path', 'viirs_path']:\n",
        "    df[col] = df[col].apply(lambda x: os.path.join(base_folder, x))\n",
        "\n",
        "df.to_csv(csv_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OOzb6WbNrwr"
      },
      "source": [
        "# dataset+loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yp4yvnIJFsTb"
      },
      "outputs": [],
      "source": [
        "class SatelliteTripletDataset(Dataset):\n",
        "    def __init__(self, csv_file, json_range_path=\"radiance_visualization_ranges.json\"):\n",
        "        self.data_info = pd.read_csv(csv_file)\n",
        "\n",
        "        with open(json_range_path, \"r\") as f:\n",
        "            self.global_ranges = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.data_info.iloc[idx]\n",
        "        goes1 = self.load_raw_radiance(record['goes1_path'])\n",
        "        goes2 = self.load_raw_radiance(record['goes2_path'])\n",
        "        viirs = self.load_raw_radiance(record['viirs_path'])\n",
        "        return (goes1, goes2), viirs\n",
        "\n",
        "    def load_raw_radiance(self, path):\n",
        "        filename = os.path.basename(path).lower()\n",
        "        is_viirs = \"viirs\" in filename or \"combined_clip\" in filename\n",
        "        sensor_type = \"VIIRS\" if is_viirs else \"GOES\"\n",
        "        band_index = 1 if is_viirs else 7\n",
        "\n",
        "        with rasterio.open(path) as src:\n",
        "            if src.count < band_index:\n",
        "                raise ValueError(f\"{path} does not contain band {band_index}\")\n",
        "            image = src.read(band_index).astype(np.float32)\n",
        "\n",
        "        # Handle NaN/Inf values\n",
        "        mask = ~(np.isnan(image) | np.isinf(image))\n",
        "        if not np.any(mask):\n",
        "            image = np.zeros_like(image)\n",
        "        else:\n",
        "            mean_val = image[mask].mean()\n",
        "            image = np.where(mask, image, mean_val)\n",
        "\n",
        "        # 🔁 Use global clipping range from JSON\n",
        "        p_low = self.global_ranges[sensor_type][\"p2\"]\n",
        "        p_high = self.global_ranges[sensor_type][\"p98\"]\n",
        "        image = np.clip(image, p_low, p_high)\n",
        "\n",
        "        return torch.from_numpy(image).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5PSrVc18F3WC"
      },
      "outputs": [],
      "source": [
        "class SatelliteDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, csv_file, batch_size=4, num_workers=0, percentile_range=(0.5, 99.5)):\n",
        "        super().__init__()\n",
        "        self.csv_file = csv_file\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.percentile_range = percentile_range\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = SatelliteTripletDataset(\n",
        "            csv_file=self.csv_file,\n",
        "            json_range_path=\"/content/radiance_visualization_ranges.json\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RHqZDRPmMOIr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7-R6rZhNvs3"
      },
      "source": [
        "# external liberery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L3_1HOshICbp",
        "outputId": "43281323-c1b8-4a32-cc25-b69d80f1330b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SwinIR'...\n",
            "remote: Enumerating objects: 333, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 333 (delta 6), reused 2 (delta 2), pack-reused 323 (from 2)\u001b[K\n",
            "Receiving objects: 100% (333/333), 29.84 MiB | 54.66 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# sys.path.append(\"C:/Users/97254/OneDrive - post.bgu.ac.il/Desktop/code4finalproj/SwinIR-main\")\n",
        "# Clone the repo\n",
        "!git clone https://github.com/JingyunLiang/SwinIR.git\n",
        "sys.path.append('/content/SwinIR')\n",
        "\n",
        "# Manually install necessary dependencies\n",
        "!pip install timm einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-P24DfpGRis",
        "outputId": "d6a854b2-aa60-46db-ffc5-49a1cc5d7ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "# Now you're safe to import\n",
        "from models.network_swinir import SwinIR\n",
        "\n",
        "model = SwinIR(\n",
        "    upscale=4,            # 100x4 = 400 → desired upscale\n",
        "    in_chans=1,           # GOES1 + GOES2 = 2 input channels\n",
        "    img_size=100,         # patch input size\n",
        "    window_size=25,        # usually 8 or 7 works fine\n",
        "    img_range=1.,\n",
        "    depths=[3,3,3,3,3,3],\n",
        "    # depths=[2,2,2,2,2,2],\n",
        "    embed_dim=180,\n",
        "    # embed_dim=64,\n",
        "    # num_heads=[8,8,8,8,8,8],\n",
        "    num_heads=[6,6,6,6,6,6],\n",
        "    mlp_ratio=2,\n",
        "    upsampler='nearest+conv',\n",
        "    # upsampler='convtranspose',\n",
        "    resi_connection='3conv'\n",
        "    # out_chans=1\n",
        "    # <<< set to 1 for single-channel output\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfcs3mYeN0Gc"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DpwLlWUniVgQ"
      },
      "outputs": [],
      "source": [
        "class SuperResolutionModule(pl.LightningModule):\n",
        "    def __init__(self, model, lr=1e-4):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.lr = lr\n",
        "\n",
        "        with open(\"/content/radiance_visualization_ranges.json\", \"r\") as f:\n",
        "            self.ranges = json.load(f)\n",
        "\n",
        "        self.goes1_path = \"/content/drive/My Drive/ONLY_TIF/ONLY_TIF/2020-11/2020-11-01_20-12/clipped_geo16.tif\"\n",
        "        self.goes2_path = \"/content/drive/My Drive/ONLY_TIF/ONLY_TIF/2020-11/2020-11-01_20-12/clipped_geo17.tif\"\n",
        "        self.viirs_path = \"/content/drive/My Drive/ONLY_TIF/ONLY_TIF/2020-11/2020-11-01_20-12/combined_clip.tif\"\n",
        "\n",
        "    def forward(self, goes1, goes2):\n",
        "        out1 = self.model(goes1)  # Each has shape [B, 1, H, W]\n",
        "        out2 = self.model(goes2)\n",
        "        return out1, out2\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        (goes1, goes2), viirs = batch\n",
        "        # print(\"in\", goes1.min(), goes1.max())\n",
        "        viirs = viirs.float()\n",
        "        # print(\"VIIRS min/max:\", viirs.min().item(), viirs.max().item())\n",
        "\n",
        "        out1, out2 = self(goes1, goes2)\n",
        "        # print(\"out\" , out1.min(), out1.max())\n",
        "\n",
        "        # out1 = out1.clamp(0., 1.)\n",
        "        # out2 = out2.clamp(0., 1.)\n",
        "\n",
        "        # print(\"out after norm\" , out1.min(), out1.max())\n",
        "\n",
        "\n",
        "        loss1 = self.criterion(out1, viirs)\n",
        "        loss2 = self.criterion(out2, viirs)\n",
        "        loss = (loss1 + loss2) / 2\n",
        "\n",
        "        psnr1 = self.compute_psnr(out1, viirs)\n",
        "        psnr2 = self.compute_psnr(out2, viirs)\n",
        "        avg_psnr = (psnr1 + psnr2) / 2\n",
        "\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_psnr', avg_psnr, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        (goes1, goes2), viirs = batch\n",
        "        viirs = viirs.float()\n",
        "\n",
        "        out1, out2 = self(goes1, goes2)\n",
        "        loss1 = self.criterion(out1, viirs)\n",
        "        loss2 = self.criterion(out2, viirs)\n",
        "        val_loss = (loss1 + loss2) / 2\n",
        "\n",
        "        psnr1 = self.compute_psnr(out1, viirs)\n",
        "        psnr2 = self.compute_psnr(out2, viirs)\n",
        "        avg_psnr = (psnr1 + psnr2) / 2\n",
        "\n",
        "        self.log('val_loss', val_loss, prog_bar=True)\n",
        "        self.log('val_psnr', avg_psnr, prog_bar=True)\n",
        "        return val_loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "    # def on_train_epoch_end(self):\n",
        "    #   (goes1, goes2), viirs = self.example_batch\n",
        "\n",
        "    #   self.eval()\n",
        "    #   with torch.no_grad():\n",
        "    #       # inputs = torch.cat((goes1, goes2), dim=1).float().to(self.device)\n",
        "    #       # output = self(inputs)\n",
        "    #       output = self(goes1.to(self.device), goes2.to(self.device))\n",
        "    #       output = F.interpolate(output, size=viirs.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "    #   inp1 = goes1[0][0].cpu().numpy()\n",
        "    #   inp2 = goes2[0][0].cpu().numpy()\n",
        "    #   tgt = viirs[0][0].cpu().numpy()\n",
        "    #   out = output[0][0].cpu().numpy()\n",
        "\n",
        "    #   fig, axs = plt.subplots(1, 4, figsize=(14, 4))\n",
        "    #   axs[0].imshow(inp1, cmap='gray'); axs[0].set_title('GOES 1')\n",
        "    #   axs[1].imshow(inp2, cmap='gray'); axs[1].set_title('GOES 2')\n",
        "    #   axs[2].imshow(tgt, cmap='gray'); axs[2].set_title('VIIRS Target')\n",
        "    #   axs[3].imshow(out, cmap='gray'); axs[3].set_title('Model Output')\n",
        "    #   fig.suptitle(f'Epoch {self.current_epoch+1} - Triplet Visualization', fontsize=14)\n",
        "\n",
        "    #   for ax in axs: ax.axis('off')\n",
        "    #   plt.tight_layout()\n",
        "    #   plt.show()\n",
        "\n",
        "\n",
        "    def on_train_start(self):\n",
        "      goes1 = self.load_radiance(self.goes1_path)\n",
        "      goes2 = self.load_radiance(self.goes2_path)\n",
        "      viirs  = self.load_radiance(self.viirs_path)\n",
        "      # Add batch dimension [1, 1, H, W]\n",
        "      self.example_batch = ((goes1.unsqueeze(0), goes2.unsqueeze(0)), viirs.unsqueeze(0))\n",
        "\n",
        "\n",
        "    def load_radiance(self, path, band=1):\n",
        "        with rasterio.open(path) as src:\n",
        "            img = src.read(band).astype(np.float32)\n",
        "            mask = ~(np.isnan(img) | np.isinf(img))\n",
        "            if mask.any():\n",
        "                img = np.where(mask, img, img[mask].mean())\n",
        "            else:\n",
        "                img = np.zeros_like(img)\n",
        "            if \"viirs\" in path.lower():\n",
        "                p2, p98 = self.ranges[\"VIIRS\"][\"p2\"], self.ranges[\"VIIRS\"][\"p98\"]\n",
        "            else:\n",
        "                p2, p98 = self.ranges[\"GOES\"][\"p2\"], self.ranges[\"GOES\"][\"p98\"]\n",
        "\n",
        "            img = np.clip(img, p2, p98)\n",
        "            # img = (img - p2) / (p98 - p2)\n",
        "            return torch.from_numpy(img).unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "    def compute_psnr(self, output, target):\n",
        "      mse = F.mse_loss(output, target)\n",
        "      if mse == 0:\n",
        "          return torch.tensor(100.0)  # Perfect match\n",
        "      max_val = self.ranges[\"VIIRS\"][\"p98\"]\n",
        "      return 20 * torch.log10(max_val / torch.sqrt(mse))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8P7doAvN2UT"
      },
      "source": [
        "#callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xKSkRRjzL6J4"
      },
      "outputs": [],
      "source": [
        "# class PSNRValidationCallback(Callback):\n",
        "#     \"\"\"\n",
        "#     Compute corrected PSNR (Kelvin metric) on a fixed month of triplets.\n",
        "#     * uses the same helpers as the baseline script              (already defined)\n",
        "#     * model gets raw GOES‑16 / GOES‑17(18) images (no bicubic up‑scale)\n",
        "#     * VIIRS GT and model prediction are both high‑res → same shape\n",
        "#     \"\"\"\n",
        "\n",
        "#     # ------------------------------------------------------------------ #\n",
        "#     def __init__(self, vis_callback, val_month_dir, every_n_epochs=1):\n",
        "#         super().__init__()\n",
        "#         self.vis_callback   = vis_callback\n",
        "#         self.val_month_dir  = val_month_dir\n",
        "#         self.every_n_epochs = every_n_epochs\n",
        "\n",
        "#         # read radiometric ranges once\n",
        "#         with open(\"/content/radiance_visualization_ranges.json\") as f:\n",
        "#             rng = json.load(f)\n",
        "#         self.vi_min = rng[\"VIIRS\"][\"p2\"]\n",
        "#         self.vi_rng = rng[\"VIIRS\"][\"p98\"] - self.vi_min\n",
        "\n",
        "#         # collect (goes1, goes2, viirs) triplets once\n",
        "#         self.triplets = self._collect_triplets(val_month_dir)\n",
        "\n",
        "#     # ------------------------------------------------------------------ #\n",
        "#     @staticmethod\n",
        "#     def _collect_triplets(root):\n",
        "#         out = []\n",
        "#         for cur, _, files in os.walk(root):\n",
        "#             files = [f for f in files if f.lower().endswith(\".tif\")]\n",
        "#             if not files:\n",
        "#                 continue\n",
        "#             g1 = g2 = v = None\n",
        "#             for f in files:\n",
        "#                 p  = os.path.join(cur, f)\n",
        "#                 lf = f.lower()\n",
        "#                 if \"geo16\" in lf:\n",
        "#                     g1 = p\n",
        "#                 elif \"geo17\" in lf or (\"geo18\" in lf and g2 is None):\n",
        "#                     g2 = p\n",
        "#                 elif \"combined\" in lf or \"viirs\" in lf:\n",
        "#                     v  = p\n",
        "#             if g1 and g2 and v:\n",
        "#                 out.append((g1, g2, v))\n",
        "#         return out\n",
        "\n",
        "#     # ------------------------------------------------------------------ #\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _load_band(path):\n",
        "#         is_viirs   = \"viirs\" in path.lower() or \"combined\" in path.lower()\n",
        "#         band_index = 1 if is_viirs else 7           # VIIRS‑I4 or GOES‑7\n",
        "#         with rasterio.open(path) as src:\n",
        "#             img = src.read(band_index).astype(np.float32)\n",
        "#         m = ~(np.isnan(img) | np.isinf(img))\n",
        "#         return np.where(m, img, img[m].mean() if m.any() else 0.0)\n",
        "\n",
        "\n",
        "#     @staticmethod\n",
        "#     def cpsnr(gt: np.ndarray, pred: np.ndarray, mask: np.ndarray = None) -> float:\n",
        "#         \"\"\"Corrected PSNR (Kelvin): brightness‑bias + clear‑pixel mask.\"\"\"\n",
        "#         if mask is None:\n",
        "#           mask = np.ones_like(gt)\n",
        "\n",
        "#         diff = (gt - pred) * mask\n",
        "#         b    = diff.sum() / (mask.sum() + 1e-8)                 # brightness bias\n",
        "#         cmse = ((gt - pred + b) ** 2 * mask).sum() / (mask.sum() + 1e-8)\n",
        "#         return -10.0 * np.log10(cmse + 1e-8)\n",
        "\n",
        "#     # ------------------------------------------------------------------ #\n",
        "#     def on_train_epoch_end(self, trainer, pl_module):\n",
        "#         epoch = trainer.current_epoch\n",
        "#         if (epoch + 1) % self.every_n_epochs:\n",
        "#             return\n",
        "\n",
        "#         psnrs = []\n",
        "\n",
        "#         for g1_path, g2_path, v_path in self.triplets:\n",
        "#             g1   = self._load_band(g1_path)\n",
        "#             g2   = self._load_band(g2_path)\n",
        "#             vi   = self._load_band(v_path)              # H×W high‑res\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 p1_t, p2_t = pl_module(\n",
        "#                     torch.from_numpy(g1)[None,None].to(pl_module.device),\n",
        "#                     torch.from_numpy(g2)[None,None].to(pl_module.device)\n",
        "#                 )\n",
        "#             p1 = p1_t.squeeze().cpu().numpy()\n",
        "#             p2 = p2_t.squeeze().cpu().numpy()\n",
        "\n",
        "#             # normalize to [0,1]\n",
        "#             vi_n = np.clip((vi - self.vi_min) / self.vi_rng,  0, 1)\n",
        "#             p1_n = np.clip((p1 - self.vi_min) / self.vi_rng,  0, 1)\n",
        "#             p2_n = np.clip((p2 - self.vi_min) / self.vi_rng,  0, 1)\n",
        "\n",
        "#             psnrs.append(self.cpsnr(vi_n, p1_n))\n",
        "#             psnrs.append(self.cpsnr(vi_n, p2_n))\n",
        "\n",
        "#         mean_psnr = float(np.mean(psnrs))\n",
        "#         if not hasattr(pl_module, \"psnr_scores\"):\n",
        "#             pl_module.psnr_scores = []\n",
        "#         pl_module.psnr_scores.append(mean_psnr)\n",
        "#         pl_module.log('val_cpsnr', mean_psnr, prog_bar=True)\n",
        "#         print(f\"📈 Epoch {epoch+1:03d} — cPSNR: {mean_psnr:.2f} dB\")\n",
        "\n",
        "#         # save the cPSNR curve\n",
        "#         save_dir = self.vis_callback.output_dir\n",
        "#         os.makedirs(save_dir, exist_ok=True)\n",
        "#         plt.figure()\n",
        "#         plt.plot(pl_module.psnr_scores, marker='o')\n",
        "#         plt.title(\"Validation cPSNR\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"dB\")\n",
        "#         plt.grid(True); plt.tight_layout()\n",
        "#         curve_path = os.path.join(save_dir, f\"psnr_curve_epoch_{epoch:03d}.png\")\n",
        "#         plt.savefig(curve_path); plt.close()\n",
        "#         print(f\"✅ Saved cPSNR curve to {curve_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "\n",
        "class PSNRValidationCallback(Callback):\n",
        "    \"\"\"\n",
        "    Compute corrected PSNR (Kelvin metric) on raw GOES-16/17 preds and on fused avg,\n",
        "    normalized by VIIRS p2/p98.\n",
        "    \"\"\"\n",
        "    def __init__(self, vis_callback, val_month_dir, every_n_epochs=1):\n",
        "        super().__init__()\n",
        "        self.vis_callback   = vis_callback\n",
        "        self.val_month_dir  = val_month_dir\n",
        "        self.every_n_epochs = every_n_epochs\n",
        "\n",
        "        # radiometric ranges\n",
        "        with open(\"/content/radiance_visualization_ranges.json\") as f:\n",
        "            rng = json.load(f)\n",
        "        self.vi_min = rng[\"VIIRS\"][\"p2\"]\n",
        "        self.vi_rng = rng[\"VIIRS\"][\"p98\"] - self.vi_min\n",
        "\n",
        "        # collect triplets\n",
        "        self.triplets = self._collect_triplets(val_month_dir)\n",
        "\n",
        "        # buffers\n",
        "        self.psnr_pred_scores  = []\n",
        "        self.psnr_fused_scores = []\n",
        "\n",
        "    @staticmethod\n",
        "    def _collect_triplets(root):\n",
        "        out = []\n",
        "        for cur, _, files in os.walk(root):\n",
        "            tifs = [f for f in files if f.lower().endswith(\".tif\")]\n",
        "            if not tifs: continue\n",
        "            g1 = g2 = v = None\n",
        "            for f in tifs:\n",
        "                p, lf = os.path.join(cur, f), f.lower()\n",
        "                if \"geo16\" in lf:   g1 = p\n",
        "                elif \"geo17\" in lf or (\"geo18\" in lf and g2 is None):\n",
        "                                     g2 = p\n",
        "                elif \"combined\" in lf or \"viirs\" in lf:\n",
        "                                     v  = p\n",
        "            if g1 and g2 and v:\n",
        "                out.append((g1, g2, v))\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_band(path):\n",
        "        is_viirs = \"viirs\" in path.lower() or \"combined\" in path.lower()\n",
        "        band_idx = 1 if is_viirs else 7\n",
        "        with rasterio.open(path) as src:\n",
        "            img = src.read(band_idx).astype(np.float32)\n",
        "        m = ~(np.isnan(img) | np.isinf(img))\n",
        "        return np.where(m, img, img[m].mean() if m.any() else 0.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def cpsnr(gt: np.ndarray, pred: np.ndarray, mask: np.ndarray = None) -> float:\n",
        "        if mask is None:\n",
        "            mask = np.ones_like(gt)\n",
        "        diff = (gt - pred) * mask\n",
        "        b    = diff.sum() / (mask.sum() + 1e-8)\n",
        "        cmse = ((gt - pred + b)**2 * mask).sum() / (mask.sum() + 1e-8)\n",
        "        return -10.0 * np.log10(cmse + 1e-8)\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        # every N epochs\n",
        "        if (trainer.current_epoch + 1) % self.every_n_epochs != 0:\n",
        "            return\n",
        "\n",
        "        psnrs_pred  = []\n",
        "        psnrs_fused = []\n",
        "\n",
        "        for g1p, g2p, vp in self.triplets:\n",
        "            # load raw\n",
        "            g1 = self._load_band(g1p)\n",
        "            g2 = self._load_band(g2p)\n",
        "            vi = self._load_band(vp)\n",
        "\n",
        "            # forward\n",
        "            t1 = torch.from_numpy(g1)[None,None].to(pl_module.device)\n",
        "            t2 = torch.from_numpy(g2)[None,None].to(pl_module.device)\n",
        "            with torch.no_grad():\n",
        "                p1_t, p2_t = pl_module(t1, t2)\n",
        "            p1 = p1_t.squeeze().cpu().numpy()\n",
        "            p2 = p2_t.squeeze().cpu().numpy()\n",
        "\n",
        "            # normalize\n",
        "            vi_n = np.clip((vi - self.vi_min) / self.vi_rng, 0, 1)\n",
        "            p1_n = np.clip((p1 - self.vi_min) / self.vi_rng, 0, 1)\n",
        "            p2_n = np.clip((p2 - self.vi_min) / self.vi_rng, 0, 1)\n",
        "\n",
        "            # cPSNR for preds\n",
        "            psnrs_pred.append(self.cpsnr(vi_n, p1_n))\n",
        "            psnrs_pred.append(self.cpsnr(vi_n, p2_n))\n",
        "\n",
        "            # fused = simple average\n",
        "            fused_n = 0.5*p1_n + 0.5*p2_n\n",
        "            psnrs_fused.append(self.cpsnr(vi_n, fused_n))\n",
        "\n",
        "        # mean over all samples\n",
        "        mean_pred  = float(np.mean(psnrs_pred))\n",
        "        mean_fused = float(np.mean(psnrs_fused))\n",
        "\n",
        "        self.psnr_pred_scores.append(mean_pred)\n",
        "        self.psnr_fused_scores.append(mean_fused)\n",
        "\n",
        "        pl_module.log('val_cpsnr_pred',  mean_pred,  prog_bar=True)\n",
        "        pl_module.log('val_cpsnr_fused', mean_fused, prog_bar=True)\n",
        "        print(f\"📈 Epoch {trainer.current_epoch+1:03d} — cPSNR(pred): {mean_pred:.2f} dB | cPSNR(fused): {mean_fused:.2f} dB\")\n",
        "\n",
        "        # save both curves in one plot\n",
        "        save_dir = self.vis_callback.output_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(self.psnr_pred_scores,  label=\"Pred1&2 avg\", marker='o')\n",
        "        plt.plot(self.psnr_fused_scores, label=\"Fused\",     marker='o')\n",
        "        plt.title(\"Validation cPSNR\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"dB\")\n",
        "        plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "        curve_path = os.path.join(save_dir, f\"psnr_curves_epoch_{trainer.current_epoch+1:03d}.png\")\n",
        "        plt.savefig(curve_path); plt.close()\n",
        "        print(f\"✅ Saved combined cPSNR plot to {curve_path}\")\n"
      ],
      "metadata": {
        "id": "JW3bBp4w3z8T"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BkY9BHVsL-OT"
      },
      "outputs": [],
      "source": [
        "# from scipy.ndimage import gaussian_filter\n",
        "# from PIL import Image\n",
        "\n",
        "# class VisualizePredictionCallback(Callback):\n",
        "#     def __init__(self, goes1_path, goes2_path, viirs_path, every_n_epochs=1):\n",
        "#         super().__init__()\n",
        "#         self.goes1_path   = goes1_path\n",
        "#         self.goes2_path   = goes2_path\n",
        "#         self.viirs_path   = viirs_path\n",
        "#         self.every_n_epochs = every_n_epochs\n",
        "\n",
        "#         # Load radiance ranges for GOES/VIIRS\n",
        "#         with open(\"/content/radiance_visualization_ranges.json\", \"r\") as f:\n",
        "#             self.ranges = json.load(f)\n",
        "\n",
        "\n",
        "#         # Make output directory with timestamp\n",
        "#         timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "#         self.output_dir = os.path.join(\"/content/checkpoints\", f\"visual_{timestamp}\")\n",
        "#         os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "#     def scale_for_visualization(self, image, sensor_type=None):\n",
        "#         p1 = np.percentile(image, 2)\n",
        "#         p99 = np.percentile(image, 98)\n",
        "#         return np.clip((image - p1) / (p99 - p1), 0, 1.0)\n",
        "\n",
        "\n",
        "#     def load_image(self, path, band=1):\n",
        "#         with rasterio.open(path) as src:\n",
        "#             image = src.read(band).astype(np.float32)\n",
        "#             mask = ~(np.isnan(image) | np.isinf(image))\n",
        "#             if np.any(mask):\n",
        "#                 mean_val = image[mask].mean()\n",
        "#                 image = np.where(mask, image, mean_val)\n",
        "#             else:\n",
        "#                 image = np.zeros_like(image)\n",
        "#             return image\n",
        "\n",
        "\n",
        "#     def on_train_epoch_end(self, trainer, pl_module):\n",
        "#         # only every N epochs\n",
        "#         if (trainer.current_epoch + 1) % self.every_n_epochs != 0:\n",
        "#             return\n",
        "\n",
        "#         # load & normalize inputs\n",
        "#         g1 = self.load_image(self.goes1_path,  7); g1_v = self.scale_for_visualization(g1)\n",
        "#         g2 = self.load_image(self.goes2_path,  7); g2_v = self.scale_for_visualization(g2)\n",
        "#         vi = self.load_image(self.viirs_path, 1); vi_v = self.scale_for_visualization(vi)\n",
        "\n",
        "#         # forward – unpack two outputs\n",
        "#         t1 = torch.from_numpy(g1).unsqueeze(0).unsqueeze(0).to(pl_module.device)\n",
        "#         t2 = torch.from_numpy(g2).unsqueeze(0).unsqueeze(0).to(pl_module.device)\n",
        "#         with torch.no_grad():\n",
        "#             pred1_t, pred2_t = pl_module(t1, t2)\n",
        "\n",
        "#         # bring back to NumPy + normalize\n",
        "#         pred1 = pred1_t.squeeze().cpu().numpy(); p1_v = self.scale_for_visualization(pred1)\n",
        "#         pred2 = pred2_t.squeeze().cpu().numpy(); p2_v = self.scale_for_visualization(pred2)\n",
        "\n",
        "#         # plot 1×5 grid\n",
        "#         imgs   = [g1_v, g2_v, vi_v, p1_v, p2_v]\n",
        "#         titles = [\"GOES-1\",\"GOES-2\",\"VIIRS\",\"Pred1\",\"Pred2\"]\n",
        "#         fig, axs = plt.subplots(1, 5, figsize=(24, 5))\n",
        "#         for ax, im_data, ttl in zip(axs, imgs, titles):\n",
        "#             im = ax.imshow(im_data, cmap=\"gray\", vmin=0, vmax=1)\n",
        "#             ax.set_title(ttl); ax.axis(\"off\")\n",
        "#             plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "#         plt.suptitle(f\"Epoch {trainer.current_epoch+1}\", fontsize=14)\n",
        "#         plt.tight_layout()\n",
        "#         out = f\"epoch_{trainer.current_epoch+1:03d}_grid.png\"\n",
        "#         plt.savefig(os.path.join(self.output_dir, out))\n",
        "#         plt.close()\n",
        "#         print(f\"✅ Saved visualization grid to {out}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizePredictionCallback(Callback):\n",
        "\n",
        "    def __init__(self, goes1_path, goes2_path, viirs_path, every_n_epochs=1):\n",
        "        super().__init__()\n",
        "        self.goes1_path   = goes1_path\n",
        "        self.goes2_path   = goes2_path\n",
        "        self.viirs_path   = viirs_path\n",
        "        self.every_n_epochs = every_n_epochs\n",
        "\n",
        "        # Load radiance ranges for GOES/VIIRS\n",
        "        with open(\"/content/radiance_visualization_ranges.json\", \"r\") as f:\n",
        "            self.ranges = json.load(f)\n",
        "\n",
        "\n",
        "        # Make output directory with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        self.output_dir = os.path.join(\"/content/checkpoints\", f\"visual_{timestamp}\")\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    def scale_for_visualization(self, image, sensor_type=None):\n",
        "        p1 = np.percentile(image, 2)\n",
        "        p99 = np.percentile(image, 98)\n",
        "        return np.clip((image - p1) / (p99 - p1), 0, 1.0)\n",
        "\n",
        "\n",
        "    def load_image(self, path, band=1):\n",
        "        with rasterio.open(path) as src:\n",
        "            image = src.read(band).astype(np.float32)\n",
        "            mask = ~(np.isnan(image) | np.isinf(image))\n",
        "            if np.any(mask):\n",
        "                mean_val = image[mask].mean()\n",
        "                image = np.where(mask, image, mean_val)\n",
        "            else:\n",
        "                image = np.zeros_like(image)\n",
        "            return image\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        if (trainer.current_epoch + 1) % self.every_n_epochs != 0:\n",
        "            return\n",
        "\n",
        "        # 1. טען ויזואליזציה (כמו שהיה)\n",
        "        g1 = self.load_image(self.goes1_path, 7);  g1_v = self.scale_for_visualization(g1)\n",
        "        g2 = self.load_image(self.goes2_path, 7);  g2_v = self.scale_for_visualization(g2)\n",
        "        vi = self.load_image(self.viirs_path,  1); vi_v = self.scale_for_visualization(vi)\n",
        "\n",
        "        # 2. תחזית\n",
        "        t1 = torch.from_numpy(g1)[None,None].to(pl_module.device)\n",
        "        t2 = torch.from_numpy(g2)[None,None].to(pl_module.device)\n",
        "        with torch.no_grad():\n",
        "            pred1_t, pred2_t = pl_module(t1, t2)\n",
        "        pred1 = pred1_t.squeeze().cpu().numpy()\n",
        "        pred2 = pred2_t.squeeze().cpu().numpy()\n",
        "        p1_v = self.scale_for_visualization(pred1)\n",
        "        p2_v = self.scale_for_visualization(pred2)\n",
        "\n",
        "        # 3. חשב fusion על ה־raw:\n",
        "        eps = 1e-8\n",
        "        sigma_var   = 5.0\n",
        "        sigma_smooth= 5.0\n",
        "        # ואריאנס\n",
        "        var1 = gaussian_filter(pred1**2, sigma=sigma_var) - gaussian_filter(pred1, sigma=sigma_var)**2\n",
        "        var2 = gaussian_filter(pred2**2, sigma=sigma_var) - gaussian_filter(pred2, sigma=sigma_var)**2\n",
        "        w1   = var1 / (var1 + var2 + eps)\n",
        "        # החליק\n",
        "        w1   = gaussian_filter(w1, sigma=sigma_smooth)\n",
        "        w1   = np.clip(w1, 0, 1)\n",
        "        # איחוד\n",
        "        fused_raw = w1 * pred1 + (1 - w1) * pred2\n",
        "        fused_viz = self.scale_for_visualization(fused_raw)\n",
        "\n",
        "        # 4. רשימת תמונות להדפסה (6 פאנלים)\n",
        "        imgs   = [g1_v, g2_v, vi_v, p1_v, p2_v, fused_viz]\n",
        "        titles = [\"GOES-1\",\"GOES-2\",\"VIIRS\",\"Pred1\",\"Pred2\",\"Fused\"]\n",
        "\n",
        "        fig, axs = plt.subplots(1, 6, figsize=(30, 5))\n",
        "        for ax, im_data, ttl in zip(axs, imgs, titles):\n",
        "            im = ax.imshow(im_data, cmap=\"gray\", vmin=0, vmax=1)\n",
        "            ax.set_title(ttl); ax.axis(\"off\")\n",
        "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "        plt.tight_layout()\n",
        "        out = f\"epoch_{trainer.current_epoch+1:03d}_grid.png\"\n",
        "        plt.savefig(os.path.join(self.output_dir, out))\n",
        "        plt.close()\n",
        "        print(f\"✅ Saved visualization grid to {out}\")\n",
        "\n",
        "        # 5. שמור את ה־fused לבד לצרכי בדיקה\n",
        "        fused_png = os.path.join(self.output_dir, f\"epoch_{trainer.current_epoch+1:03d}_fused.png\")\n",
        "        Image.fromarray((fused_viz * 255).astype(np.uint8)).save(fused_png)\n",
        "        print(f\"✅ Saved fused image to {fused_png}\")"
      ],
      "metadata": {
        "id": "w1zydUd8xVC_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOukz6qTN5GH"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AuA2vlo4GmnM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "pl_model = SuperResolutionModule(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dvUrezC519W-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9df65c-9a7f-41fc-a259-f5a622cbfd93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOES-1 shape: torch.Size([1, 1, 100, 100])\n",
            "GOES-2 shape: torch.Size([1, 1, 100, 100])\n",
            "Output from GOES-1 shape: torch.Size([1, 1, 400, 400])\n",
            "Output from GOES-2 shape: torch.Size([1, 1, 400, 400])\n"
          ]
        }
      ],
      "source": [
        "# === Create dummy inputs ===\n",
        "goes1 = torch.randn(1, 1, 100, 100)  # GOES-1 image\n",
        "goes2 = torch.randn(1, 1, 100, 100)  # GOES-2 image\n",
        "\n",
        "# === Forward pass through the updated SwinIR wrapper ===\n",
        "pl_model.eval()\n",
        "with torch.no_grad():\n",
        "    out1, out2 = pl_model(goes1, goes2)\n",
        "\n",
        "pred1 = out1.squeeze().cpu().numpy()\n",
        "pred2 = out2.squeeze().cpu().numpy()\n",
        "\n",
        "# === Print shapes ===\n",
        "print(\"GOES-1 shape:\", goes1.shape)\n",
        "print(\"GOES-2 shape:\", goes2.shape)\n",
        "print(\"Output from GOES-1 shape:\", out1.shape)\n",
        "print(\"Output from GOES-2 shape:\", out2.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJecnx6tMBe_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902,
          "referenced_widgets": [
            "ad8b6dc5ff9d46c387ee5b5db63566ff",
            "400b97fef28f41d6925539672726f642",
            "66e3fedab36d403387151d14fbfe677c",
            "75ecf466e8e64559a4c845c784191430",
            "f68c63121f7d4a9b8eabab71f69188a7",
            "db9455ef60ff425296c5432c4a7a51ad",
            "c2166e1cf96e40378919c9b13291856d",
            "548297bb4ff145d3a74164447705691c",
            "88f4cf81028844448a6e887c12a2769f",
            "c7ea19ced4994dda99322acdb7a6a011",
            "5fe65a8e66ef4db88fbf849050c8522f"
          ]
        },
        "outputId": "fefdbb52-3cfd-426f-a603-dd59732e7229"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type   | Params | Mode\n",
            "--------------------------------------------\n",
            "0 | model     | SwinIR | 6.2 M  | eval\n",
            "1 | criterion | L1Loss | 0      | eval\n",
            "--------------------------------------------\n",
            "6.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "6.2 M     Total params\n",
            "24.859    Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "359       Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad8b6dc5ff9d46c387ee5b5db63566ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Epoch 001 — cPSNR(pred): 12.42 dB | cPSNR(fused): 12.46 dB\n",
            "✅ Saved combined cPSNR plot to /content/checkpoints/visual_2025-05-07_13-11-48/psnr_curves_epoch_001.png\n",
            "✅ Saved visualization grid to epoch_001_grid.png\n",
            "✅ Saved fused image to /content/checkpoints/visual_2025-05-07_13-11-48/epoch_001_fused.png\n",
            "📈 Epoch 002 — cPSNR(pred): 12.42 dB | cPSNR(fused): 12.46 dB\n",
            "✅ Saved combined cPSNR plot to /content/checkpoints/visual_2025-05-07_13-11-48/psnr_curves_epoch_002.png\n",
            "✅ Saved visualization grid to epoch_002_grid.png\n",
            "✅ Saved fused image to /content/checkpoints/visual_2025-05-07_13-11-48/epoch_002_fused.png\n",
            "📈 Epoch 003 — cPSNR(pred): 12.42 dB | cPSNR(fused): 12.46 dB\n",
            "✅ Saved combined cPSNR plot to /content/checkpoints/visual_2025-05-07_13-11-48/psnr_curves_epoch_003.png\n",
            "✅ Saved visualization grid to epoch_003_grid.png\n",
            "✅ Saved fused image to /content/checkpoints/visual_2025-05-07_13-11-48/epoch_003_fused.png\n",
            "📈 Epoch 004 — cPSNR(pred): 12.42 dB | cPSNR(fused): 12.46 dB\n",
            "✅ Saved combined cPSNR plot to /content/checkpoints/visual_2025-05-07_13-11-48/psnr_curves_epoch_004.png\n",
            "✅ Saved visualization grid to epoch_004_grid.png\n",
            "✅ Saved fused image to /content/checkpoints/visual_2025-05-07_13-11-48/epoch_004_fused.png\n",
            "📈 Epoch 005 — cPSNR(pred): 12.42 dB | cPSNR(fused): 12.46 dB\n",
            "✅ Saved combined cPSNR plot to /content/checkpoints/visual_2025-05-07_13-11-48/psnr_curves_epoch_005.png\n",
            "✅ Saved visualization grid to epoch_005_grid.png\n",
            "✅ Saved fused image to /content/checkpoints/visual_2025-05-07_13-11-48/epoch_005_fused.png\n",
            "📈 Epoch 006 — cPSNR(pred): 12.42 dB | cPSNR(fused): 12.46 dB\n",
            "✅ Saved combined cPSNR plot to /content/checkpoints/visual_2025-05-07_13-11-48/psnr_curves_epoch_006.png\n",
            "✅ Saved visualization grid to epoch_006_grid.png\n",
            "✅ Saved fused image to /content/checkpoints/visual_2025-05-07_13-11-48/epoch_006_fused.png\n",
            "📈 Epoch 007 — cPSNR(pred): 12.42 dB | cPSNR(fused): 12.46 dB\n",
            "✅ Saved combined cPSNR plot to /content/checkpoints/visual_2025-05-07_13-11-48/psnr_curves_epoch_007.png\n",
            "✅ Saved visualization grid to epoch_007_grid.png\n",
            "✅ Saved fused image to /content/checkpoints/visual_2025-05-07_13-11-48/epoch_007_fused.png\n"
          ]
        }
      ],
      "source": [
        "# === DataModule ===\n",
        "datamodule = SatelliteDataModule(\n",
        "    csv_file=\"/content/drive/My Drive/superres_triplets.csv\",\n",
        "    batch_size=1,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# === Visualization Callback ===\n",
        "vis_callback = VisualizePredictionCallback(\n",
        "    goes1_path=\"/content/drive/My Drive/ONLY_TIF/ONLY_TIF/2020-11/2020-11-01_20-12/clipped_geo16.tif\",\n",
        "    goes2_path=\"/content/drive/My Drive/ONLY_TIF/ONLY_TIF/2020-11/2020-11-01_20-12/clipped_geo17.tif\",\n",
        "    viirs_path=\"/content/drive/My Drive/ONLY_TIF/ONLY_TIF/2020-11/2020-11-01_20-12/combined_clip.tif\",\n",
        "    every_n_epochs=1\n",
        ")\n",
        "\n",
        "\n",
        "# === PSNR Callback ===\n",
        "psnr_callback = PSNRValidationCallback(\n",
        "    vis_callback=vis_callback,\n",
        "    val_month_dir=\"/content/drive/My Drive/ONLY_TIF/ONLY_TIF/2023-02\",  # Folder with validation triplets\n",
        "    every_n_epochs=1\n",
        ")\n",
        "\n",
        "# === Logger ===\n",
        "logger = CSVLogger(\"logs\", name=\"swinir_superres\")\n",
        "\n",
        "# === Trainer ===\n",
        "trainer = Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    precision=16,                # mixed precision\n",
        "    logger=logger,\n",
        "    gradient_clip_val=1.0,\n",
        "    callbacks=[psnr_callback, vis_callback],\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# === Fit the model ===\n",
        "trainer.fit(pl_model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# comb 2 img results to 1"
      ],
      "metadata": {
        "id": "6XBfCfu1AhQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from scipy.ndimage import gaussian_filter\n",
        "# from PIL import Image\n",
        "# import matplotlib.pyplot as plt\n",
        "# import tifffile\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "# def laplacian_pyramid(img, levels):\n",
        "#     # Gaussian pyramid\n",
        "#     gp = [img.astype(np.float32)]\n",
        "#     for i in range(levels):\n",
        "#         gp.append(cv2.pyrDown(gp[-1]))\n",
        "#     # Laplacian pyramid\n",
        "#     lp = [gp[-1]]\n",
        "#     for i in range(levels, 0, -1):\n",
        "#         ge = cv2.pyrUp(gp[i], dstsize=gp[i-1].shape[:2][::-1])\n",
        "#         lp.append(gp[i-1] - ge)\n",
        "#     return lp\n",
        "\n",
        "# def reconstruct_from_laplacian(lp):\n",
        "#     img = lp[0]\n",
        "#     for level in lp[1:]:\n",
        "#         img = cv2.pyrUp(img, dstsize=level.shape[:2][::-1]) + level\n",
        "#     return img\n",
        "\n",
        "# def fuse_via_laplacian(pred1, pred2, levels=4):\n",
        "#     lp1 = laplacian_pyramid(pred1, levels)\n",
        "#     lp2 = laplacian_pyramid(pred2, levels)\n",
        "#     # שילוב – כאן ממוצע, אפשר גם לבחור max abs\n",
        "#     lp_fused = [(l1 + l2) / 2 for l1, l2 in zip(lp1, lp2)]\n",
        "#     return reconstruct_from_laplacian(lp_fused)\n",
        "\n",
        "# # שימוש:\n",
        "# fused_raw = fuse_via_laplacian(pred1, pred2, levels=4)\n",
        "\n",
        "\n",
        "# # 4. שמירה\n",
        "# os.makedirs(\"results\", exist_ok=True)\n",
        "# tifffile.imwrite(\"results/fused_raw1.tif\", fused_raw.astype(np.float32))\n",
        "\n",
        "# fmin, fmax = fused_raw.min(), fused_raw.max()\n",
        "# fused_vis  = (fused_raw - fmin) / (fmax - fmin)\n",
        "# fused_uint8 = (np.clip(fused_vis, 0, 1) * 255).astype(np.uint8)\n",
        "# Image.fromarray(fused_uint8).save(\"results/fused_preview1.png\")\n",
        "\n",
        "# print(\"✅ Saved fused_raw.tif and fused_preview.png in ./results/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COgpRCGUAgej",
        "outputId": "9680731d-4e0c-439b-b4e5-e47c1f7fbcb9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved fused_raw.tif and fused_preview.png in ./results/\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad8b6dc5ff9d46c387ee5b5db63566ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_400b97fef28f41d6925539672726f642",
              "IPY_MODEL_66e3fedab36d403387151d14fbfe677c",
              "IPY_MODEL_75ecf466e8e64559a4c845c784191430"
            ],
            "layout": "IPY_MODEL_f68c63121f7d4a9b8eabab71f69188a7"
          }
        },
        "400b97fef28f41d6925539672726f642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9455ef60ff425296c5432c4a7a51ad",
            "placeholder": "​",
            "style": "IPY_MODEL_c2166e1cf96e40378919c9b13291856d",
            "value": "Epoch 7:  94%"
          }
        },
        "66e3fedab36d403387151d14fbfe677c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_548297bb4ff145d3a74164447705691c",
            "max": 1260,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88f4cf81028844448a6e887c12a2769f",
            "value": 1180
          }
        },
        "75ecf466e8e64559a4c845c784191430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ea19ced4994dda99322acdb7a6a011",
            "placeholder": "​",
            "style": "IPY_MODEL_5fe65a8e66ef4db88fbf849050c8522f",
            "value": " 1180/1260 [17:17&lt;01:10,  1.14it/s, v_num=3, train_loss=nan.0, train_psnr=nan.0, val_cpsnr_pred=12.40, val_cpsnr_fused=12.50]"
          }
        },
        "f68c63121f7d4a9b8eabab71f69188a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "db9455ef60ff425296c5432c4a7a51ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2166e1cf96e40378919c9b13291856d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "548297bb4ff145d3a74164447705691c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f4cf81028844448a6e887c12a2769f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7ea19ced4994dda99322acdb7a6a011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe65a8e66ef4db88fbf849050c8522f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}